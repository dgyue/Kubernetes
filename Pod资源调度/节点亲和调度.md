# 节点亲和调度

​	节点亲和是调度程序用来确定Pod对象调度位置的调度法则，这些规则基于节点上的自定义标签和Pod对象上指定的标签选择器进行定义，而支持这种调度机制的有NodeName和NodeAffinity调度插件。简单来说，节点亲和调度机制支持Pod资源定义自身对期望运行的某类节点的倾向性，倾向于运行指定类型的节点即为“亲和”关系，否则为“反亲和”关系。

​	在Pod上定义节点亲和规则时有两种类型的节点亲和关系：强制（required）亲和和首选（preferred）亲和，或分别称为硬亲和与软亲和。强制亲和限定了调度Pod资源时必须要满足的规则，无可用节点时Pod对象会被置为Pending状态，直到满足规则的节点出现。相比较来说，首选规则实现的是一种柔和调度限制，它同样倾向于将Pod运行在某类特定的节点之上，但无法满足调度需求时，调度器将选择一个无法匹配规则的节点，而非将Pod置于Pending状态。

​	在Pod规范上定义节点亲和性和规则的关键点有两个：一是给节点规划并配置合乎期望的标签；二是为Pod对象定义合理的标签选择器。正如preferredDuringSchedulingIgnoredDuringExecution和requiredDuringSchedulingIgnoredDuringExecution字段名中的后半段字符串IgnoredDuringExecution隐含的意义所指，在Pod资源基于节点亲和规则调度至某节点之后，因节点标签发生了改变而变得不再符合Pod定义的亲和规则时，调度器也不会将Pod从此节点上移出，因而亲和性调度仅在调度执行的过程中进行一次即时的判断，而非持续地监视亲和规则是否能够得以满足。

 <img src="/Users/apple/Library/Application Support/typora-user-images/image-20210701221117118.png" alt="image-20210701221117118" style="zoom:50%;" />



## Pod节点选择器

​	Pod资源可以使用.spec.nodeName直接指定要运行的目标节点，也可以基于.spec.nodeSelector指定的标签选择器过滤符合条件的节点作为可用目标节点，最终选择则基于打分机制完成。因此，后者也称为节点选择器。用户事先为特定部分的Node资源对象设定好标签，而后即可配置Pod通过节点选择器实现类似于节点的强制亲和调度。

​	由kubeadm部署的Kubernetes集群默认会为每个节点附加kubernetes.io/arch、kubernetes.io/hostname和kubernetes.io/os等标签，而且主节点还会有一个node-role.kubernetes.io/master标签，其中kubernetes.io/hostname适合NodeName类型的调度。无法满足nodeSelector的调度需求时，我们还可以使用kubectl label nodes/NODE命令为其附加自定义标签。

```shell
# kubectl label nodes/k8s-node01.ilinux.io gpu=
nodes/k8s-node01.ilinux.io labeled
# kubectl label nodes/k8s-node03.ilinux.io gpu=
nodes/k8s-node03.ilinux.io labeled
# kubectl get nodes -l 'gpu' -o custom-columns=NAME:.metadata.name
NAME
k8s-node01.ilinux.io
k8s-node03.ilinux.io
```

示例：定义Pod资源使用节点选择器定义节点亲和机制，它倾向于运行在拥有GPU设备的节点上。

```shell
apiVersion: v1
kind: Pod
metadata:
  name: pod-with-nodeselector
spec:
  containers:
  - name: demoapp
    image: ikubernetes.demoapp:v1.0
  nodeSelector:
    gpu: ''

#按照计划，pod/pod-with-nodeselector资源仅可能会运行在节点k8s-node01或k8s-node03上
# kubectl apply -f pod-with-nodeselector.yaml
pod/pod-with-nodeselectorcreated
# kubectl get pod/pod-with-nodeselector -o jsonpath={.spec.nodeName}
k8s-node03.ilinux.io
```

​	事实上，多数情况下用户都无需关心Pod对象的具体运行位置，除非Pod依赖的特殊条件仅能由部分节点满足时，例如GPU和SSD等。即便如此，也应该尽量避免使用.spec.nodeName静态指定Pod对象的运行位置，而是应该让调度器基于标签和标签选择器为Pod挑选匹配的工作节点。另外，Pod规范中的.spec.nodeSelector仅支持简单等值关系的节点选择器，而.spec.affinity.nodeAffinity支持更灵活的节点选择器表达式，而且可以实现硬亲和与软亲和逻辑。



## 强制节点亲和

​	Pod规范中的.spec.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution字段用于定义节点的强制亲和关系，它的值是一个对象列表，可由一到多个nodeSelectorTerms对象组成，彼此间为“逻辑或”关系。nodeSelectorTerms用于定义节点选择器，其值为对象列表，它支持matchExpressions和matchFields两种复杂的表达机制。

- ​	matchExpressions：标签选择器表达式，基于节点标签进行过滤；可重复使用以表达不同的匹配条件，各条件间为“或”关系。
- ​	matchFields：以字段选择器表达的节点选择器；可重复使用以表达不同的匹配条件，各条件间为“或”关系。

​	每个匹配条件可由一到多个匹配规则组成，例如某个matchExpressions条件下可同时存在两个表达式规则，如下面示例所示，同一条件下的各条规则彼此间为“逻辑与”关系。这意味着某节点满足nodeSelectorTerms中的任一个条件即可，但满足某个条件指的是完全匹配该条件下定义的所有规则。

​	下面配置清单示例中，Pod模版使用了强制节点亲和约束，它要求Pod只能运行在那些拥有gpu标签且不具有node-role.kubernetes.io/master标签的节点之上。

```shell
apiVersion: apps/v1
kind: Deployment
metadata:
  name: node-affinity-required
  namespace: default
spec:
  replicas: 5
  selector:
    matchLabels:
      app: demoapp
      ctlr: node-affinity-required
  template:
    metadata:
      labels:
        app: demoapp
        ctlr: node-affinity-required
    spec:
      containers:
      - name: demoapp
        image: ikubernetes/demoapp:v1.0
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerm:
            - matchExpressions:
              - key: gpu
                operator: Exists
              - key: node-role.kubernetes.io/master
                operator: DoesNotExist
```

​	目前环境中，k8s-node01和k8s-node03设定了gpu标签，而k8s-master01拥有主节点标识的标签。因此按照期望，5个Pod副本仅会运行在k8s-node01和k8s-node03上。

```shell
# kubectl apply -f node-affinity-required-demo.yaml
deployment.apps/node-affinity-required created
# kubectl get pods -l ctlr=node-affinity-required -o custom-columns=NAME:.metadata.name,NODE:.spce.nodeName
node-affinity-required-5c469987c-2fdq1	k8s-node03.ilinux.io
node-affinity-required-5c469987c-hfcvn	k8s-node01.ilinux.io
node-affinity-required-5c469987c-mt4gg	k8s-node03.ilinux.io
node-affinity-required-5c469987c-pm56j	k8s-node01.ilinux.io
node-affinity-required-5c469987c-vwbtt	k8s-node03.ilinux.io
```

​	另外，调度器调度Pod时，支撑节点亲和机制的NodeName和NodeAffinity等插件仅是其中组调度条件，kubernetes配置的其他插件依然会参与到Pod的调度过程。例如可以为Pod模板中的demoapp容器添加如下资源后重新进行测试来验证调度器的工作模型。

```shell
containers:
- name: demoapp
  image: ikubernetes/demoapp:v1.0
  resources:
    requests:
      cpu: 2
      memory: 2Gi
```

​	注册到Filter扩展点的调度插件NodeResourcesFit负责检查节点的可分配资源是否能容纳Pod的资源请求，计算方式是节点上的资源总量减去已运行在该节点上的所有Pod对象的requests资源量之和。



## 首选节点亲和

​	节点首选亲和机制为节点选择机制提供了一种柔和性控制逻辑，被调度的Pod对象不再是“必须”，而是“应该”放置到某些特定节点之上，但条件不满足时，该Pod也能够接受被编排到其他不符合条件的节点之上。另外，多个软亲和条件并存时，它还支持为每个条件定义weight属性以区别它们优先级，取值范围是1～100，数字越大优先级越高。

```shell
apiVersion: apps/v1
kind: Deployment
metadata:
  name: node-affinity-preferred
spec:
  replicas: 5
  selector:
    matchLabels:
      app: demoapp
      ctlr: node-affinity-preferred
  template:
    metadata:
      name: demoapp
      labels:
        app: demoapp
        ctlr: node-affinity-preferred
    spec:
      containers:
      - name: demoapp
        image: ikubernetes/demoapp:v1.0
        resources:
          requests:
            cpu: 1500m
            memory: 1Gi
      affinity:
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 60
            preference:
              matchExpressions:
              - key: gpu
                operator: Exists
          - weight: 30
            preference:
              matchExpressions:
              - key: zone
                operator: In
                values: ["foo","bar"]
```

​	示例中Pod资源模板定义了节点软亲和，以选择尽量运行在指定范围内拥有gpu标签或者zone标签的节点之上，其中gpu标签是更为重要的倾向性规则，它的权重为60，相比较来说zone标签的重要性低了一级，因为它的权重为30。这么一来，如果集群中拥有足够多的节点，它将被此规则分为4类：在指定范围内拥有gpu标签和zone标签、仅满足gpu一个标签条件、仅满足zone一个标签条件，以及不满足任何标签筛选条件的节点。

